{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7447509,"sourceType":"datasetVersion","datasetId":4334995}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nx = np.load('/kaggle/input/brain-eeg-spectrograms/EEG_Spectrograms/1007356722.npy')\nprint(x.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,random_split\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom torch.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom torchvision.transforms import v2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:52:11.117127Z","iopub.execute_input":"2025-04-11T14:52:11.117387Z","iopub.status.idle":"2025-04-11T14:52:24.809182Z","shell.execute_reply.started":"2025-04-11T14:52:11.117358Z","shell.execute_reply":"2025-04-11T14:52:24.808345Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nclass EEGSpectrogramDataset(Dataset):\n    def __init__(self, eeg_ids, spectrogram_dict, label_dict):\n        self.eeg_ids = eeg_ids\n        self.spectrogram_dict = spectrogram_dict\n        self.label_dict = label_dict\n\n        self.transform = v2.Compose([\n            v2.Resize((224, 224)),\n            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    def __len__(self):\n        return len(self.eeg_ids)\n    \n    def __getitem__(self, idx):\n        eeg_id = self.eeg_ids[idx]\n        spectrogram = self.spectrogram_dict[eeg_id]\n        \n        label_str = self.label_dict[eeg_id]\n        label = self._label_to_index(label_str)\n\n        stacked = np.vstack([spectrogram[..., i] for i in range(4)])[..., np.newaxis]\n        stacked = torch.tensor(stacked, dtype=torch.float32).permute(2, 0, 1)\n    \n        # spectrogram = torch.tensor(spectrogram, dtype=torch.float32).permute(2, 0, 1)\n\n        # spectrogram = F.interpolate(spectrogram.unsqueeze(0), \n        #                          size=(224, 224), \n        #                          mode='bilinear',\n        #                          align_corners=False).squeeze(0)\n        \n        # if self.transform:\n            # spectrogram = self.transform(spectrogram)\n\n        rgb = torch.cat([stacked, stacked, stacked], dim=0)\n        \n        # Resize to 224x224 using bilinear interpolation\n        rgb = F.interpolate(rgb.unsqueeze(0), \n                          size=(224, 224), \n                          mode='bilinear',\n                          align_corners=False).squeeze(0)\n        \n        # Apply transforms\n        rgb = self.transform(rgb)\n        \n        label = torch.tensor(label, dtype=torch.long)\n        \n        return rgb, label\n    \n    def _label_to_index(self, label_str):\n        label_mapping = {\n            'Seizure': 0,\n            'LPD': 1,\n            'GPD': 2,\n            'LRDA': 3,\n            'GRDA': 4,\n            'Other': 5\n        }\n        return label_mapping.get(label_str, 5)\n\nspectrogram_dict = np.load('/kaggle/input/brain-eeg-spectrograms/eeg_specs.npy', allow_pickle=True).item()\n\ntrain_df = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\nlabel_dict = dict(zip(train_df['eeg_id'], train_df['expert_consensus']))\n\ncommon_eeg_ids = [eeg_id for eeg_id in spectrogram_dict.keys() if eeg_id in label_dict]\n\nprint(f\"Total EEG IDs in spectrogram dict: {len(spectrogram_dict)}\")\nprint(f\"Total EEG IDs in labels: {len(label_dict)}\")\nprint(f\"Common EEG IDs with both: {len(common_eeg_ids)}\")\nprint(f\"Sample spectrogram shape: {spectrogram_dict[common_eeg_ids[0]].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:52:27.085190Z","iopub.execute_input":"2025-04-11T14:52:27.085916Z","iopub.status.idle":"2025-04-11T14:53:47.619012Z","shell.execute_reply.started":"2025-04-11T14:52:27.085891Z","shell.execute_reply":"2025-04-11T14:53:47.618210Z"}},"outputs":[{"name":"stdout","text":"Total EEG IDs in spectrogram dict: 17089\nTotal EEG IDs in labels: 17089\nCommon EEG IDs with both: 17089\nSample spectrogram shape: (128, 256, 4)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_eeg_ids, test_eeg_ids = train_test_split(\n    common_eeg_ids,\n    test_size=0.3,\n    random_state=42,\n    stratify=[label_dict[eeg_id] for eeg_id in common_eeg_ids]\n)\n\ntrain_dataset = EEGSpectrogramDataset(train_eeg_ids, spectrogram_dict, label_dict)\ntest_dataset = EEGSpectrogramDataset(test_eeg_ids, spectrogram_dict, label_dict)\n\nbatch_size = 16\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n    drop_last=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n\nprint(\"\\nDataset successfully created:\")\nprint(f\"Total samples: {len(common_eeg_ids)}\")\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Sample spectrogram shape: {train_dataset[0][0].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:20:19.614663Z","iopub.execute_input":"2025-04-11T15:20:19.615036Z","iopub.status.idle":"2025-04-11T15:20:19.648879Z","shell.execute_reply.started":"2025-04-11T15:20:19.615010Z","shell.execute_reply":"2025-04-11T15:20:19.648101Z"}},"outputs":[{"name":"stdout","text":"\nDataset successfully created:\nTotal samples: 17089\nTrain samples: 11962\nTest samples: 5127\nBatch size: 16\nSample spectrogram shape: torch.Size([3, 224, 224])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"num_classes = 6 \n\nmodel = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=num_classes)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\nscheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-7)\n\nos.makedirs(\"/kaggle/working/saved_models3\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:20:35.975486Z","iopub.execute_input":"2025-04-11T15:20:35.975868Z","iopub.status.idle":"2025-04-11T15:20:36.581576Z","shell.execute_reply.started":"2025-04-11T15:20:35.975846Z","shell.execute_reply":"2025-04-11T15:20:36.580932Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:20:39.510280Z","iopub.execute_input":"2025-04-11T15:20:39.510926Z","iopub.status.idle":"2025-04-11T15:20:39.560538Z","shell.execute_reply.started":"2025-04-11T15:20:39.510898Z","shell.execute_reply":"2025-04-11T15:20:39.559815Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:06:10.138983Z","iopub.execute_input":"2025-04-11T14:06:10.139533Z","iopub.status.idle":"2025-04-11T14:06:10.143956Z","shell.execute_reply.started":"2025-04-11T14:06:10.139506Z","shell.execute_reply":"2025-04-11T14:06:10.143336Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=3, verbose=False):\n        self.patience = patience\n        self.counter = 0\n        self.best_loss = float('inf')\n        self.early_stop = False\n        self.verbose = verbose\n\n    def __call__(self, val_loss):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"EarlyStopping: {self.counter}/{self.patience} without improvement.\")\n            if self.counter >= self.patience:\n                self.early_stop = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:20:40.951943Z","iopub.execute_input":"2025-04-11T15:20:40.952601Z","iopub.status.idle":"2025-04-11T15:20:40.959679Z","shell.execute_reply.started":"2025-04-11T15:20:40.952567Z","shell.execute_reply":"2025-04-11T15:20:40.958772Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"test size: 0.3, le = 2e-4","metadata":{}},{"cell_type":"code","source":"# Set this to True to use FP16 training\nfp16 = True\nscaler = GradScaler(enabled=fp16)\n\nnum_epochs = 20\nbest_acc = 0.0\nbest_test_loss = float('inf')\nuse_accuracy_for_best = False  # Set to False to save best model by val loss\n\nearly_stopper = EarlyStopping(patience=3, verbose=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', enabled=fp16):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    scheduler.step()\n    \n    train_acc = correct / total\n    avg_train_loss = running_loss / len(train_loader)\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n\n    # ---------------- Validation Phase ----------------\n    model.eval()\n    test_loss, correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast(device_type='cuda', enabled=fp16):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            test_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    test_acc = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n    # ---------------- Save Current Epoch Model ----------------\n    model_path = f\"/kaggle/working/saved_models3/swin_epoch_{epoch+1}.pth\"\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved: {model_path}\")\n\n    # ---------------- Save Best Model ----------------\n    if use_accuracy_for_best:\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models3/swin_best_model.pth\")\n            # print(\"Best model updated based on highest accuracy!\")\n    else:\n        if avg_test_loss < best_test_loss:\n            best_test_loss = avg_test_loss\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models3/swin_best_model.pth\")\n            # print(\"Best model updated based on lowest validation loss!\")\n\n    early_stopper(avg_test_loss)\n    if early_stopper.early_stop:\n        print(\"Early stopping triggered. Ending training.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:20:52.427975Z","iopub.execute_input":"2025-04-11T15:20:52.428261Z","iopub.status.idle":"2025-04-11T15:39:12.573147Z","shell.execute_reply.started":"2025-04-11T15:20:52.428242Z","shell.execute_reply":"2025-04-11T15:39:12.572146Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [1/20]\nTrain Loss: 1.3932, Train Accuracy: 0.4874\nTest Loss: 1.1476, Test Accuracy: 0.5793\nModel saved: /kaggle/working/saved_models3/swin_epoch_1.pth\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 1/20 [02:02<38:42, 122.25s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [2/20]\nTrain Loss: 1.1051, Train Accuracy: 0.5980\nTest Loss: 1.0482, Test Accuracy: 0.6269\nModel saved: /kaggle/working/saved_models3/swin_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 2/20 [04:04<36:42, 122.34s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [3/20]\nTrain Loss: 0.9679, Train Accuracy: 0.6511\nTest Loss: 0.9040, Test Accuracy: 0.6696\nModel saved: /kaggle/working/saved_models3/swin_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 3/20 [06:06<34:39, 122.33s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [4/20]\nTrain Loss: 0.8720, Train Accuracy: 0.6879\nTest Loss: 0.8541, Test Accuracy: 0.6977\nModel saved: /kaggle/working/saved_models3/swin_epoch_4.pth\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 4/20 [08:09<32:36, 122.31s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [5/20]\nTrain Loss: 0.7936, Train Accuracy: 0.7195\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [10:11<30:33, 122.23s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9100, Test Accuracy: 0.6710\nModel saved: /kaggle/working/saved_models3/swin_epoch_5.pth\nEarlyStopping: 1/3 without improvement.\n\nEpoch [6/20]\nTrain Loss: 0.7012, Train Accuracy: 0.7523\nTest Loss: 0.8029, Test Accuracy: 0.7100\nModel saved: /kaggle/working/saved_models3/swin_epoch_6.pth\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6/20 [12:13<28:32, 122.32s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [7/20]\nTrain Loss: 0.6033, Train Accuracy: 0.7863\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 7/20 [14:15<26:28, 122.17s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8165, Test Accuracy: 0.7135\nModel saved: /kaggle/working/saved_models3/swin_epoch_7.pth\nEarlyStopping: 1/3 without improvement.\n\nEpoch [8/20]\nTrain Loss: 0.4760, Train Accuracy: 0.8358\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 8/20 [16:17<24:26, 122.19s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9015, Test Accuracy: 0.7152\nModel saved: /kaggle/working/saved_models3/swin_epoch_8.pth\nEarlyStopping: 2/3 without improvement.\n\nEpoch [9/20]\nTrain Loss: 0.3547, Train Accuracy: 0.8819\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 8/20 [18:20<27:30, 137.52s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9514, Test Accuracy: 0.7088\nModel saved: /kaggle/working/saved_models3/swin_epoch_9.pth\nEarlyStopping: 3/3 without improvement.\nEarly stopping triggered. Ending training.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"lr = 3e-4","metadata":{}},{"cell_type":"code","source":"# Set this to True to use FP16 training\nfp16 = True\nscaler = GradScaler(enabled=fp16)\n\nnum_epochs = 20\nbest_acc = 0.0\nbest_test_loss = float('inf')\nuse_accuracy_for_best = False  # Set to False to save best model by val loss\n\nearly_stopper = EarlyStopping(patience=3, verbose=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', enabled=fp16):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    scheduler.step()\n    \n    train_acc = correct / total\n    avg_train_loss = running_loss / len(train_loader)\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n\n    # ---------------- Validation Phase ----------------\n    model.eval()\n    test_loss, correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast(device_type='cuda', enabled=fp16):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            test_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    test_acc = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n    # ---------------- Save Current Epoch Model ----------------\n    model_path = f\"/kaggle/working/saved_models2/swin_epoch_{epoch+1}.pth\"\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved: {model_path}\")\n\n    # ---------------- Save Best Model ----------------\n    if use_accuracy_for_best:\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models2/swin_best_model.pth\")\n            # print(\"Best model updated based on highest accuracy!\")\n    else:\n        if avg_test_loss < best_test_loss:\n            best_test_loss = avg_test_loss\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models2/swin_best_model.pth\")\n            # print(\"Best model updated based on lowest validation loss!\")\n\n    early_stopper(avg_test_loss)\n    if early_stopper.early_stop:\n        print(\"Early stopping triggered. Ending training.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:01:58.309931Z","iopub.execute_input":"2025-04-11T15:01:58.310646Z","iopub.status.idle":"2025-04-11T15:19:41.630866Z","shell.execute_reply.started":"2025-04-11T15:01:58.310620Z","shell.execute_reply":"2025-04-11T15:19:41.629787Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [1/20]\nTrain Loss: 1.2066, Train Accuracy: 0.5612\nTest Loss: 1.0466, Test Accuracy: 0.6281\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 1/20 [02:13<42:09, 133.11s/it]","output_type":"stream"},{"name":"stdout","text":"Model saved: /kaggle/working/saved_models2/swin_epoch_1.pth\n\nEpoch [2/20]\nTrain Loss: 0.9868, Train Accuracy: 0.6440\nTest Loss: 0.9246, Test Accuracy: 0.6703\nModel saved: /kaggle/working/saved_models2/swin_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 2/20 [04:26<39:55, 133.10s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [3/20]\nTrain Loss: 0.8768, Train Accuracy: 0.6876\nTest Loss: 0.8728, Test Accuracy: 0.6808\nModel saved: /kaggle/working/saved_models2/swin_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 3/20 [06:39<37:43, 133.12s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [4/20]\nTrain Loss: 0.7928, Train Accuracy: 0.7140\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 4/20 [08:52<35:28, 133.01s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8994, Test Accuracy: 0.6831\nModel saved: /kaggle/working/saved_models2/swin_epoch_4.pth\nEarlyStopping: 1/3 without improvement.\n\nEpoch [5/20]\nTrain Loss: 0.7106, Train Accuracy: 0.7472\nTest Loss: 0.8278, Test Accuracy: 0.7048\nModel saved: /kaggle/working/saved_models2/swin_epoch_5.pth\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [11:05<33:14, 132.96s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [6/20]\nTrain Loss: 0.6107, Train Accuracy: 0.7845\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6/20 [13:17<31:00, 132.90s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8679, Test Accuracy: 0.6951\nModel saved: /kaggle/working/saved_models2/swin_epoch_6.pth\nEarlyStopping: 1/3 without improvement.\n\nEpoch [7/20]\nTrain Loss: 0.4827, Train Accuracy: 0.8320\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 7/20 [15:30<28:46, 132.83s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8893, Test Accuracy: 0.7168\nModel saved: /kaggle/working/saved_models2/swin_epoch_7.pth\nEarlyStopping: 2/3 without improvement.\n\nEpoch [8/20]\nTrain Loss: 0.3451, Train Accuracy: 0.8828\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 7/20 [17:43<32:54, 151.90s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9999, Test Accuracy: 0.7101\nModel saved: /kaggle/working/saved_models2/swin_epoch_8.pth\nEarlyStopping: 3/3 without improvement.\nEarly stopping triggered. Ending training.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"lr = 1e-4","metadata":{}},{"cell_type":"code","source":"# Set this to True to use FP16 training\nfp16 = True\nscaler = GradScaler(enabled=fp16)\n\nnum_epochs = 20\nbest_acc = 0.0\nbest_test_loss = float('inf')\nuse_accuracy_for_best = False  # Set to False to save best model by val loss\n\nearly_stopper = EarlyStopping(patience=3, verbose=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', enabled=fp16):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    scheduler.step()\n    \n    train_acc = correct / total\n    avg_train_loss = running_loss / len(train_loader)\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n\n    # ---------------- Validation Phase ----------------\n    model.eval()\n    test_loss, correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast(device_type='cuda', enabled=fp16):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            test_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    test_acc = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n    # ---------------- Save Current Epoch Model ----------------\n    model_path = f\"/kaggle/working/saved_models/swin_epoch_{epoch+1}.pth\"\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved: {model_path}\")\n\n    # ---------------- Save Best Model ----------------\n    if use_accuracy_for_best:\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models/swin_best_model.pth\")\n            # print(\"Best model updated based on highest accuracy!\")\n    else:\n        if avg_test_loss < best_test_loss:\n            best_test_loss = avg_test_loss\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models/swin_best_model.pth\")\n            # print(\"Best model updated based on lowest validation loss!\")\n\n    early_stopper(avg_test_loss)\n    if early_stopper.early_stop:\n        print(\"Early stopping triggered. Ending training.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:06:19.680467Z","iopub.execute_input":"2025-04-11T14:06:19.680740Z","iopub.status.idle":"2025-04-11T14:21:43.152824Z","shell.execute_reply.started":"2025-04-11T14:06:19.680720Z","shell.execute_reply":"2025-04-11T14:21:43.151950Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [1/20]\nTrain Loss: 1.2177, Train Accuracy: 0.5572\nTest Loss: 1.0966, Test Accuracy: 0.6047\nModel saved: /kaggle/working/saved_models/swin_epoch_1.pth\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 1/20 [02:13<42:13, 133.32s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [2/20]\nTrain Loss: 0.9386, Train Accuracy: 0.6601\nTest Loss: 0.8937, Test Accuracy: 0.6849\nModel saved: /kaggle/working/saved_models/swin_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 2/20 [04:25<39:44, 132.45s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [3/20]\nTrain Loss: 0.8163, Train Accuracy: 0.7073\nTest Loss: 0.8253, Test Accuracy: 0.7115\nModel saved: /kaggle/working/saved_models/swin_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 3/20 [06:36<37:26, 132.17s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [4/20]\nTrain Loss: 0.7325, Train Accuracy: 0.7361\nTest Loss: 0.7914, Test Accuracy: 0.7177\nModel saved: /kaggle/working/saved_models/swin_epoch_4.pth\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 4/20 [08:48<35:12, 132.04s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [5/20]\nTrain Loss: 0.6458, Train Accuracy: 0.7682\nTest Loss: 0.8573, Test Accuracy: 0.6984\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [11:00<32:58, 131.88s/it]","output_type":"stream"},{"name":"stdout","text":"Model saved: /kaggle/working/saved_models/swin_epoch_5.pth\nEarlyStopping: 1/3 without improvement.\n\nEpoch [6/20]\nTrain Loss: 0.5343, Train Accuracy: 0.8105\nTest Loss: 0.8663, Test Accuracy: 0.7106\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6/20 [13:11<30:44, 131.76s/it]","output_type":"stream"},{"name":"stdout","text":"Model saved: /kaggle/working/saved_models/swin_epoch_6.pth\nEarlyStopping: 2/3 without improvement.\n\nEpoch [7/20]\nTrain Loss: 0.4065, Train Accuracy: 0.8566\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6/20 [15:23<35:54, 153.91s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8804, Test Accuracy: 0.7232\nModel saved: /kaggle/working/saved_models/swin_epoch_7.pth\nEarlyStopping: 3/3 without improvement.\nEarly stopping triggered. Ending training.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Set this to True to use FP16 training\nfp16 = True\nscaler = GradScaler(enabled=fp16)\n\nnum_epochs = 20\nbest_acc = 0.0\nbest_test_loss = float('inf')\nuse_accuracy_for_best = False  # Set to False to save best model by val loss\n\nearly_stopper = EarlyStopping(patience=3, verbose=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', enabled=fp16):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    scheduler.step()\n    \n    train_acc = correct / total\n    avg_train_loss = running_loss / len(train_loader)\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n\n    # ---------------- Validation Phase ----------------\n    model.eval()\n    test_loss, correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast(device_type='cuda', enabled=fp16):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            test_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    test_acc = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n    # ---------------- Save Current Epoch Model ----------------\n    model_path = f\"/kaggle/working/saved_models/swin_epoch_{epoch+1}.pth\"\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved: {model_path}\")\n\n    # ---------------- Save Best Model ----------------\n    if use_accuracy_for_best:\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models/swin_best_model.pth\")\n            # print(\"Best model updated based on highest accuracy!\")\n    else:\n        if avg_test_loss < best_test_loss:\n            best_test_loss = avg_test_loss\n            torch.save(model.state_dict(), \"/kaggle/working/saved_models/swin_best_model.pth\")\n            # print(\"Best model updated based on lowest validation loss!\")\n\n    early_stopper(avg_test_loss)\n    if early_stopper.early_stop:\n        print(\"Early stopping triggered. Ending training.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T04:13:40.358437Z","iopub.execute_input":"2025-04-10T04:13:40.359148Z","iopub.status.idle":"2025-04-10T04:26:53.349847Z","shell.execute_reply.started":"2025-04-10T04:13:40.359123Z","shell.execute_reply":"2025-04-10T04:26:53.348821Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [1/20]\nTrain Loss: 1.1359, Train Accuracy: 0.5890\nTest Loss: 0.9194, Test Accuracy: 0.6635\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 1/20 [02:12<42:03, 132.80s/it]","output_type":"stream"},{"name":"stdout","text":"Model saved: /kaggle/working/saved_models/swin_epoch_1.pth\n\nEpoch [2/20]\nTrain Loss: 0.8681, Train Accuracy: 0.6912\nTest Loss: 0.9150, Test Accuracy: 0.6849\nModel saved: /kaggle/working/saved_models/swin_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 2/20 [04:25<39:44, 132.48s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [3/20]\nTrain Loss: 0.7645, Train Accuracy: 0.7301\nTest Loss: 0.7847, Test Accuracy: 0.7159\nModel saved: /kaggle/working/saved_models/swin_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 3/20 [06:37<37:29, 132.30s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [4/20]\nTrain Loss: 0.6679, Train Accuracy: 0.7629\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 4/20 [08:48<35:13, 132.12s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.7990, Test Accuracy: 0.7180\nModel saved: /kaggle/working/saved_models/swin_epoch_4.pth\nEarlyStopping: 1/3 without improvement.\n\nEpoch [5/20]\nTrain Loss: 0.5578, Train Accuracy: 0.8023\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [11:01<33:01, 132.12s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8116, Test Accuracy: 0.7285\nModel saved: /kaggle/working/saved_models/swin_epoch_5.pth\nEarlyStopping: 2/3 without improvement.\n\nEpoch [6/20]\nTrain Loss: 0.4231, Train Accuracy: 0.8529\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [13:12<39:38, 158.60s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9393, Test Accuracy: 0.6975\nModel saved: /kaggle/working/saved_models/swin_epoch_6.pth\nEarlyStopping: 3/3 without improvement.\nEarly stopping triggered. Ending training.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}