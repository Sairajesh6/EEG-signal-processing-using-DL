{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11276583,"sourceType":"datasetVersion","datasetId":7049806},{"sourceId":11304731,"sourceType":"datasetVersion","datasetId":7069794}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch import einsum\nfrom tqdm import tqdm\nfrom einops import rearrange, repeat\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.metrics import confusion_matrix, classification_report\ntry:\n    from torchinfo import summary\nexcept:\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:04:50.776082Z","iopub.execute_input":"2025-04-07T11:04:50.776490Z","iopub.status.idle":"2025-04-07T11:04:50.784485Z","shell.execute_reply.started":"2025-04-07T11:04:50.776456Z","shell.execute_reply":"2025-04-07T11:04:50.782947Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class PatchEmbedding3D(nn.Module):\n    \n    def __init__(self, in_channels:int, embedding_dim:int, patch_size:int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Conv3d(\n            in_channels, embedding_dim,\n            kernel_size=(1, patch_size, patch_size),\n            stride=(1, patch_size, patch_size)\n        )\n    \n    def forward(self, x):\n        x = x.permute(0, 2, 1, 3, 4).contiguous()   \n        \n        x = self.proj(x)\n\n        B, D, T, H, W = x.shape\n        x = rearrange(x, 'b d t h w -> b (t h w) d', b = B, d = D, t = T, h = H, w = W)\n        \n        return x              ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:04:53.344854Z","iopub.execute_input":"2025-04-07T11:04:53.345215Z","iopub.status.idle":"2025-04-07T11:04:53.351876Z","shell.execute_reply.started":"2025-04-07T11:04:53.345188Z","shell.execute_reply":"2025-04-07T11:04:53.350528Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n\n    def __init__(self, embedding_dim: int, eps:float=10**-6) -> None:\n        \n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(embedding_dim)) \n        self.bias = nn.Parameter(torch.zeros(embedding_dim)) \n\n    def forward(self, x):\n\n        mean = x.mean(dim = -1, keepdim = True)\n        std = x.std(dim = -1, keepdim = True) \n        \n        return self.alpha * (x - mean) / (std + self.eps) + self.bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:04:55.372334Z","iopub.execute_input":"2025-04-07T11:04:55.372744Z","iopub.status.idle":"2025-04-07T11:04:55.378979Z","shell.execute_reply.started":"2025-04-07T11:04:55.372713Z","shell.execute_reply":"2025-04-07T11:04:55.377739Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):\n    \n    def __init__(self, embedding_dim: int, mlp_size: int, dropout: float):\n        \n        super().__init__()\n        self.linear_1 = nn.Linear(embedding_dim, mlp_size) \n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(mlp_size, embedding_dim) \n\n    def forward(self, x):\n        return self.linear_2(self.dropout(F.gelu(self.linear_1(x))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:04:57.764109Z","iopub.execute_input":"2025-04-07T11:04:57.764531Z","iopub.status.idle":"2025-04-07T11:04:57.770868Z","shell.execute_reply.started":"2025-04-07T11:04:57.764496Z","shell.execute_reply":"2025-04-07T11:04:57.769403Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Attention(nn.Module):\n\n    def __init__(self, embedding_dim: int, num_heads: int):\n\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.d_k = embedding_dim // num_heads\n        assert embedding_dim % num_heads == 0, \"d_model must be divisible by h\"\n\n        self.qkv = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n        self.w_o = nn.Linear(embedding_dim, embedding_dim, bias=False)\n\n    def forward(self, x):\n        \n        B, N, D = x.shape\n        \n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, D // self.num_heads).permute(2, 0, 3, 1, 4)\n        query, key, value = qkv[0], qkv[1], qkv[2]\n        \n        attention = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n        attention = F.softmax(attention, dim=-1)\n\n        x = (attention @ value).transpose(1, 2).reshape(B, N, D)\n        \n        return self.w_o(x)        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:04:59.435101Z","iopub.execute_input":"2025-04-07T11:04:59.435483Z","iopub.status.idle":"2025-04-07T11:04:59.443430Z","shell.execute_reply.started":"2025-04-07T11:04:59.435448Z","shell.execute_reply":"2025-04-07T11:04:59.441929Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n\n    def __init__(self, embedding_dim:int, num_heads: int, mlp_size: int, batch_size: int, num_frames: int, num_patches: int, dropout: float):\n        \n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.mlp_size = mlp_size\n        self.batch_size = batch_size\n        self.num_frames = num_frames\n        self.num_patches = num_patches\n        self.dropout = nn.Dropout(dropout)\n\n        self.norm1 = LayerNormalization(embedding_dim)\n        self.spatial_attention = Attention(embedding_dim, num_heads)\n        self.temporal_attention = Attention(embedding_dim, num_heads)\n        self.dropout_1 = nn.Dropout(dropout)\n        \n        self.norm2 = LayerNormalization(embedding_dim)  \n        self.ffn = FeedForwardBlock(embedding_dim, mlp_size, dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        \n\n    def forward(self, x): \n\n        B = self.batch_size\n        T = self.num_frames\n        S = self.num_patches\n        \n        residual = x\n        x = self.norm1(x)\n        \n        X = x[:,1:,:]\n        \n        init_cls_token = x[:,0,:].unsqueeze(1)\n        \n        # Spatial Attention\n        t_cls_token = init_cls_token.repeat(1, T, 1)\n        t_cls_token = rearrange(t_cls_token, 'b t m -> (b t) m', b = B, t = T).unsqueeze(1)\n\n        xs = rearrange(X, 'b (s t) d -> (b t) s d', b = B, t = T, s = S)\n        xs = torch.cat((t_cls_token, xs), 1)\n\n        spatial_attention = self.spatial_attention(xs)\n\n        t_cls_token = spatial_attention[:,0,:]\n        t_cls_token = rearrange(t_cls_token, '(b t) m -> b t m', b = B, t = T)\n        t_cls_token = torch.mean(t_cls_token, 1, True)\n\n        spatial_attention = spatial_attention[:,1:,:]\n        spatial_attention = rearrange(spatial_attention, '(b t) s d -> b (t s) d', b = B, t = T, s = S)\n        spatial_attention = torch.cat((t_cls_token, spatial_attention), 1)\n        \n        # Temporal Attention\n        s_cls_token = init_cls_token.repeat(1, S, 1)\n        s_cls_token = rearrange(s_cls_token, 'b s m -> (b s) m', b = B, s = S).unsqueeze(1)\n\n        xt = rearrange(X, 'b (s t) d -> (b s) t d', b = B, s = S, t = T)\n        xt = torch.cat((s_cls_token, xt), 1)\n\n        temporal_attention = self.temporal_attention(xt)\n        \n        s_cls_token = temporal_attention[:,0,:]\n        s_cls_token = rearrange(s_cls_token, '(b s) m -> b s m', b = B, s = S)\n        s_cls_token = torch.mean(s_cls_token, 1, True)\n\n        temporal_attention = temporal_attention[:,1:,:]\n        temporal_attention = rearrange(temporal_attention, '(b s) t d -> b (t s) d', b = B, t = T, s = S)\n        temporal_attention = torch.cat((s_cls_token, temporal_attention),1)\n        \n\n        combined = spatial_attention + temporal_attention\n        attn_out = combined + residual\n        attn_out = self.dropout_1(attn_out)\n        \n        out = attn_out + self.dropout_2(self.ffn(self.norm2(attn_out)))\n        \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:05:01.191715Z","iopub.execute_input":"2025-04-07T11:05:01.192066Z","iopub.status.idle":"2025-04-07T11:05:01.204557Z","shell.execute_reply.started":"2025-04-07T11:05:01.192037Z","shell.execute_reply":"2025-04-07T11:05:01.203272Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self, embedding_dim: int, num_layers: int, num_heads: int, mlp_size: int, batch_size: int, num_frames: int, num_patches: int, dropout: float = 0.1):\n\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderBlock(\n                embedding_dim = embedding_dim,\n                num_heads = num_heads,\n                mlp_size = mlp_size,\n                batch_size = batch_size,\n                num_frames = num_frames,\n                num_patches = num_patches,\n                dropout = dropout\n            ) for _ in range(num_layers)\n        ])\n        self.norm = LayerNormalization(embedding_dim) \n\n    def forward(self, x):\n\n        for layer in self.layers:\n            x = layer(x)\n\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:05:08.090191Z","iopub.execute_input":"2025-04-07T11:05:08.090555Z","iopub.status.idle":"2025-04-07T11:05:08.097078Z","shell.execute_reply.started":"2025-04-07T11:05:08.090528Z","shell.execute_reply":"2025-04-07T11:05:08.095866Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class SpatioTemporalTransformer(nn.Module):\n\n    def __init__(self, \n                 feature_size:int = 64,\n                 num_frames:int = 50,\n                 batch_size:int = 16,\n                 in_channels:int = 1,\n                 patch_size:int = 16,\n                 num_layers:int = 12,\n                 embedding_dim:int = 768,\n                 mlp_size:int = 3072,\n                 num_heads:int = 12,\n                 dropout:float = 0.1,\n                 embed_dropout:float = 0.1,\n                 num_classes:int = 6):    \n        super().__init__()\n\n        assert feature_size % patch_size == 0, f'Image size {feature_size} is not divisible by the patch size {patch_size}'\n\n        self.num_patches = (feature_size * feature_size) // patch_size**2\n        self.batch_size = batch_size\n        self.num_frames = num_frames\n        \n        self.class_embedding = nn.Parameter(data = torch.randn(1, 1, embedding_dim), requires_grad = True)\n        \n        self.patch_embedding = PatchEmbedding3D(\n            in_channels = in_channels, \n            embedding_dim = embedding_dim, \n            patch_size = patch_size\n        )\n        \n        self.positional_embedding = nn.Parameter(data = torch.randn(1, (self.num_patches * self.num_frames) + 1, embedding_dim), requires_grad = True)\n        \n        self.embedding_dropout = nn.Dropout(embed_dropout)\n        \n        self.transformer_encoder = Encoder(\n            embedding_dim = embedding_dim,\n            num_layers = num_layers,\n            num_heads = num_heads,\n            mlp_size = mlp_size,\n            batch_size = batch_size,\n            num_frames = num_frames,\n            num_patches = self.num_patches,\n            dropout = dropout\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(in_features = embedding_dim, out_features = num_classes)\n        )\n\n\n    def forward(self, x):\n        \n        x = self.patch_embedding(x)\n        \n        class_token = self.class_embedding.expand(self.batch_size, -1, -1)\n        \n        x = torch.cat((class_token, x), dim=1)\n        \n        x = self.positional_embedding + x\n        \n        x = self.embedding_dropout(x)\n        \n        x = self.transformer_encoder(x)\n\n        x = self.classifier(x[:,0,:])\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T11:07:13.600528Z","iopub.execute_input":"2025-04-07T11:07:13.601009Z","iopub.status.idle":"2025-04-07T11:07:13.615567Z","shell.execute_reply.started":"2025-04-07T11:07:13.600963Z","shell.execute_reply":"2025-04-07T11:07:13.614431Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, npz_files):\n        self.npz_files = npz_files\n\n    def __len__(self):\n        return len(self.npz_files)\n\n    def __getitem__(self, idx):\n        data = np.load(self.npz_files[idx])\n        features = data['features']         \n        label = data['label']      \n        features = torch.tensor(features, dtype=torch.float32)\n        label = torch.tensor(label, dtype=torch.long).squeeze()\n        return features, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T03:03:09.714139Z","iopub.execute_input":"2025-04-07T03:03:09.714418Z","iopub.status.idle":"2025-04-07T03:03:09.719423Z","shell.execute_reply.started":"2025-04-07T03:03:09.714397Z","shell.execute_reply":"2025-04-07T03:03:09.718565Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dir = \"/kaggle/input/eeg-spatio-temporal-feature-map-dataset/Train/\"\nval_dir = \"/kaggle/input/eeg-spatio-temporal-feature-map-dataset/Validation/\"\ntest_dir = \"/kaggle/input/eeg-spatio-temporal-feature-map-dataset/Test/\"\n\ntrain_files = [os.path.join(train_dir, f) for f in sorted(os.listdir(train_dir)) if f.endswith(\".npz\")]\ntest_files = [os.path.join(test_dir, f) for f in sorted(os.listdir(test_dir)) if f.endswith(\".npz\")]\nval_files = [os.path.join(val_dir, f) for f in sorted(os.listdir(val_dir)) if f.endswith(\".npz\")]\n\ntrain_dataset = EEGDataset(train_files)\ntest_dataset = EEGDataset(test_files)\nval_dataset = EEGDataset(val_files)\n\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T03:03:11.742857Z","iopub.execute_input":"2025-04-07T03:03:11.743165Z","iopub.status.idle":"2025-04-07T03:03:12.017658Z","shell.execute_reply.started":"2025-04-07T03:03:11.743140Z","shell.execute_reply":"2025-04-07T03:03:12.016780Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Trainer:\n    \n    def __init__(self, model, train_loader, test_loader, device, config):\n        \n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n        self.device = device\n        self.config = config\n        \n        self.criterion = nn.CrossEntropyLoss()\n        \n        self.optimizer = optim.AdamW(\n            model.parameters(),\n            lr = config['learning_rate'],\n            betas = (0.9, 0.999),\n            weight_decay = 1e-2\n        )\n        \n        total_steps = config['num_epochs'] * len(train_loader)\n        warmup_steps = int(config.get('warmup_ratio', 0.1) * total_steps)\n        \n        self.scheduler = get_linear_schedule_with_warmup(\n            self.optimizer,\n            num_warmup_steps = warmup_steps,\n            num_training_steps = total_steps\n        )\n        \n        self.train_loss_history = []\n        self.test_loss_history = []\n        self.train_acc_history = []\n        self.test_acc_history = []\n    \n        self.save_dir = config['save_dir']\n        os.makedirs(self.save_dir, exist_ok=True)\n\n    \n    def train_epoch(self):\n        \n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (data, targets) in enumerate(tqdm(self.train_loader, desc = \"Train\")):\n            data = data.to(self.device)\n            targets = targets.to(self.device)\n            \n            outputs = self.model(data)\n            loss = self.criterion(outputs, targets)\n            \n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            self.scheduler.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n        \n        epoch_loss = running_loss / len(self.train_loader)\n        epoch_acc = 100. * correct / total\n        \n        self.train_loss_history.append(epoch_loss)\n        self.train_acc_history.append(epoch_acc)\n        \n        return epoch_loss, epoch_acc\n\n    \n    def test_epoch(self):\n        \n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for batch_idx, (data, targets) in enumerate(tqdm(self.test_loader, desc = \"Test\")):\n                data = data.to(self.device)\n                targets = targets.to(self.device)\n                \n                outputs = self.model(data)\n                loss = self.criterion(outputs, targets)\n                \n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        \n        epoch_loss = running_loss / len(self.test_loader)\n        epoch_acc = 100. * correct / total\n        \n        self.test_loss_history.append(epoch_loss)\n        self.test_acc_history.append(epoch_acc)\n        \n        return epoch_loss, epoch_acc\n\n    \n    def save_checkpoint(self, epoch):\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'train_loss_history': self.train_loss_history,\n            'test_loss_history': self.test_loss_history,\n            'train_acc_history': self.train_acc_history,\n            'test_acc_history': self.test_acc_history,\n            'config': self.config\n        }\n        \n        torch.save(checkpoint, os.path.join(self.save_dir, f'checkpoint_epoch_{epoch}.pth'))\n            \n        metrics = {\n            'train_loss': self.train_loss_history,\n            'test_loss': self.test_loss_history,\n            'train_acc': self.train_acc_history,\n            'test_acc': self.test_acc_history\n        }\n        with open(os.path.join(self.save_dir, 'training_metrics.json'), 'w') as f:\n            json.dump(metrics, f)\n\n    \n    def train(self, num_epochs):\n        \n        start_time = time.time()\n        \n        for epoch in range(4, num_epochs + 1):\n            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n            \n            train_loss, train_acc = self.train_epoch()\n            test_loss, test_acc = self.test_epoch()\n            \n            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n            print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n\n            self.save_checkpoint(epoch)\n            \n        total_time = time.time() - start_time\n        print(f\"\\nTraining completed in {total_time//60:.0f}m {total_time%60:.0f}s\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:01:21.923422Z","iopub.execute_input":"2025-04-07T05:01:21.923760Z","iopub.status.idle":"2025-04-07T05:01:21.936791Z","shell.execute_reply.started":"2025-04-07T05:01:21.923698Z","shell.execute_reply":"2025-04-07T05:01:21.935806Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"config = {\n    'feature_size': 64,\n    'num_frames': 50,\n    'batch_size': 16,\n    'in_channels': 1,\n    'patch_size': 16,\n    'num_layers': 6, # 12\n    'embedding_dim': 768,\n    'mlp_size': 3072,\n    'num_heads': 12,\n    'dropout': 0.1,\n    'embed_dropout': 0.1,\n    'num_classes': 6,\n    'learning_rate': 1e-3,\n    'save_dir': '/kaggle/working/checkpoints',\n    'num_epochs': 7,\n    'warmup_ratio': 0.1\n}\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = SpatioTemporalTransformer(\n    feature_size = config['feature_size'],\n    num_frames = config['num_frames'],\n    batch_size = config['batch_size'],\n    in_channels = config['in_channels'],\n    patch_size = config['patch_size'],\n    num_layers = config['num_layers'],\n    embedding_dim = config['embedding_dim'],\n    mlp_size = config['mlp_size'],\n    num_heads = config['num_heads'],\n    dropout = config['dropout'],\n    embed_dropout = config['embed_dropout'],\n    num_classes = config['num_classes']\n)\n    \ntrainer = Trainer(\n    model = model,\n    train_loader = train_loader,\n    test_loader = test_loader,\n    device = device,\n    config = config\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:54:09.472346Z","iopub.execute_input":"2025-04-07T04:54:09.472668Z","iopub.status.idle":"2025-04-07T04:54:10.075876Z","shell.execute_reply.started":"2025-04-07T04:54:09.472645Z","shell.execute_reply":"2025-04-07T04:54:10.074990Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def load_checkpoint(path, model, optimizer = None, device = 'cpu'):\n    checkpoint = torch.load(path, map_location = device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch'], checkpoint['train_loss_history'], checkpoint['test_loss_history'], checkpoint['train_acc_history'], checkpoint['test_acc_history']\n\n\n# model = SpatioTemporalTransformer(...)\n# optimizer = optim.Adam(model.parameters(), lr = config['learning_rate'], betas = (0.9, 0.999), weight_decay = 0.1)\noptimizer = optim.AdamW(\n            model.parameters(),\n            lr = config['learning_rate'],\n            betas = (0.9, 0.999),\n            weight_decay = 1e-2\n        )\n\nepoch, train_loss, test_loss, train_acc, test_acc = load_checkpoint('/kaggle/input/checkpoint3/checkpoint_epoch_3.pth', model, optimizer, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:01:58.012581Z","iopub.execute_input":"2025-04-07T05:01:58.012909Z","iopub.status.idle":"2025-04-07T05:02:03.375094Z","shell.execute_reply.started":"2025-04-07T05:01:58.012883Z","shell.execute_reply":"2025-04-07T05:02:03.374116Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-26-85ad7e5e758e>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(path, map_location = device)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:02:10.104612Z","iopub.execute_input":"2025-04-07T05:02:10.104975Z","iopub.status.idle":"2025-04-07T05:02:10.110047Z","shell.execute_reply.started":"2025-04-07T05:02:10.104947Z","shell.execute_reply":"2025-04-07T05:02:10.109346Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"trainer.train(config['num_epochs'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:02:16.849192Z","iopub.execute_input":"2025-04-07T05:02:16.849480Z","iopub.status.idle":"2025-04-07T07:15:33.192694Z","shell.execute_reply.started":"2025-04-07T05:02:16.849458Z","shell.execute_reply":"2025-04-07T07:15:33.191661Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 4/7\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1800/1800 [30:38<00:00,  1.02s/it]\nTest: 100%|██████████| 450/450 [02:38<00:00,  2.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7525 | Train Acc: 22.66%\nTest Loss: 1.8130 | Test Acc: 14.64%\n\nEpoch 5/7\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1800/1800 [30:18<00:00,  1.01s/it]\nTest: 100%|██████████| 450/450 [03:47<00:00,  1.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7935 | Train Acc: 17.92%\nTest Loss: 1.7614 | Test Acc: 19.43%\n\nEpoch 6/7\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1800/1800 [31:25<00:00,  1.05s/it]\nTest: 100%|██████████| 450/450 [03:05<00:00,  2.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7455 | Train Acc: 23.24%\nTest Loss: 1.7236 | Test Acc: 25.35%\n\nEpoch 7/7\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1800/1800 [28:48<00:00,  1.04it/s]\nTest: 100%|██████████| 450/450 [02:29<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7484 | Train Acc: 22.83%\nTest Loss: 1.7325 | Test Acc: 24.11%\n\nTraining completed in 133m 16s\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!zip -r checkpoints_123.zip checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:46:11.425185Z","iopub.execute_input":"2025-04-07T04:46:11.425490Z","iopub.status.idle":"2025-04-07T04:48:01.410762Z","shell.execute_reply.started":"2025-04-07T04:46:11.425467Z","shell.execute_reply":"2025-04-07T04:48:01.409860Z"}},"outputs":[{"name":"stdout","text":"  adding: checkpoints/ (stored 0%)\n  adding: checkpoints/checkpoint_epoch_3.pth (deflated 8%)\n  adding: checkpoints/checkpoint_epoch_2.pth (deflated 8%)\n  adding: checkpoints/checkpoint_epoch_1.pth (deflated 8%)\n  adding: checkpoints/training_metrics.json (deflated 45%)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from IPython.display import FileLink \nFileLink(r'checkpoints_123.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:48:27.530337Z","iopub.execute_input":"2025-04-07T04:48:27.530766Z","iopub.status.idle":"2025-04-07T04:48:27.538286Z","shell.execute_reply.started":"2025-04-07T04:48:27.530724Z","shell.execute_reply":"2025-04-07T04:48:27.537320Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoints_123.zip","text/html":"<a href='checkpoints_123.zip' target='_blank'>checkpoints_123.zip</a><br>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import os\nimport shutil\n\nfolder_path = '/kaggle/working/checkpoints'\n\nif os.path.exists(folder_path):\n    if os.path.isdir(folder_path):\n        shutil.rmtree(folder_path)  # Deletes the folder and all its contents\n        print(f\"{folder_path} has been deleted.\")\n    else:\n        print(f\"{folder_path} is not a directory.\")\nelse:\n    print(f\"{folder_path} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:53:33.264595Z","iopub.execute_input":"2025-04-07T04:53:33.264981Z","iopub.status.idle":"2025-04-07T04:53:33.396593Z","shell.execute_reply.started":"2025-04-07T04:53:33.264956Z","shell.execute_reply":"2025-04-07T04:53:33.395573Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/checkpoints has been deleted.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import os\n\nfile_path = '/kaggle/working/checkpoints_123.zip'\n\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(f\"{file_path} has been deleted.\")\nelse:\n    print(f\"{file_path} does not exist.\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:53:36.185344Z","iopub.execute_input":"2025-04-07T04:53:36.185660Z","iopub.status.idle":"2025-04-07T04:53:36.419313Z","shell.execute_reply.started":"2025-04-07T04:53:36.185633Z","shell.execute_reply":"2025-04-07T04:53:36.418543Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/checkpoints_123.zip has been deleted.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def load_checkpoint(path, model, optimizer = None, device = 'cpu'):\n    checkpoint = torch.load(path, map_location = device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch'], checkpoint['train_loss_history'], checkpoint['test_loss_history'], checkpoint['train_acc_history'], checkpoint['test_acc_history']\n\n\n# model = SpatioTemporalTransformer(...)\n# optimizer = optim.Adam(model.parameters(), lr = config['learning_rate'], betas = (0.9, 0.999), weight_decay = 0.1)\n\nepoch, train_loss, test_loss, train_acc, test_acc = load_checkpoint('/kaggle/working/checkpoints/.. .pth', model, optimizer, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_model(model, val_loader, device, num_classes = 6):\n    \n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    all_preds = []\n    all_labels = []\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        for batch_idx, (data, targets) in enumerate(tqdm(val_loader, desc=\"Validation\")):\n            \n            data = data.to(device)\n            targets = targets.to(device)\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(targets.cpu().numpy())\n    \n    val_loss /= len(val_loader)\n    val_acc = 100. * correct / total\n    \n    print(f\"\\nValidation Results:\")\n    print(f\"Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=[f'Class {i}' for i in range(num_classes)]))\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=[f'Class {i}' for i in range(num_classes)],\n                yticklabels=[f'Class {i}' for i in range(num_classes)])\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    \n    return val_loss, val_acc\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model = SpatioTemporalTransformer(...)\n\nval_loss, val_acc = validate_model(model, val_loader, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}